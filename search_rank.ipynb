{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4903e8",
   "metadata": {},
   "source": [
    "## Imports & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b9591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04437908",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"n_queries\": 5_000,\n",
    "    \"bm25_top_k\": 5,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "set_seed(CONFIG[\"seed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38ad7b",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d983f47",
   "metadata": {},
   "source": [
    "Search engines are one of the core application domains of Natural Language Processing (NLP), as they require understanding, matching, and ranking text according to a user’s information need. In real-world systems, the challenge goes beyond retrieving relevant documents, it primarily lies in ranking them correctly, since users typically interact only with the top few results.\n",
    "\n",
    "Traditional lexical retrieval models, such as TF-IDF and BM25, have long served as strong baselines due to their efficiency, simplicity, and scalability. HoIver, these methods rely on surface-level term matching and often fail to capture deeper semantic relationships betIen queries and documents. Recent advances in NLP, particularly with Transformer-based models, have enabled the development of neural ranking models that can better model contextual meaning and semantic similarity.\n",
    "\n",
    "In this work, I implement and evaluate a modern search ranking pipeline that combines classical lexical retrieval with neural re-ranking using Transformer models. Using the MS MARCO Passage Ranking dataset, I compare three approaches: a BM25 baseline, a zero-shot neural re-ranking model, and a fine-tuned neural re-ranker. The goal is to quantitatively assess the impact of neural re-ranking and supervised fine-tuning using standard industry metrics, namely MRR@10 and NDCG@10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f60045",
   "metadata": {},
   "source": [
    "## Dataset: MS MARCO Passage Ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f341cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 100%|██████████| 10047/10047 [00:00<00:00, 106982.72 examples/s]\n",
      "Generating train split: 100%|██████████| 82326/82326 [00:00<00:00, 177357.83 examples/s]\n",
      "Generating test split: 100%|██████████| 9650/9650 [00:00<00:00, 190468.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face cache location\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/nunes/huggingface_cache/huggingface\"\n",
    "\n",
    "dataset = load_dataset(\"ms_marco\", \"v1.1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb8a57",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1552f99",
   "metadata": {},
   "source": [
    "MS MARCO is query-centric: each example contains a query with multiple candidate passages and binary relevance labels.\n",
    "\n",
    "I explicitly flatten this structure into (query, document, label) pairs to enable ranking models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005c961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dataset_split(dataset_split, max_queries: int):\n",
    "    rows = []\n",
    "\n",
    "    for i, example in enumerate(dataset_split):\n",
    "        if i >= max_queries:\n",
    "            break\n",
    "\n",
    "        query_id = example[\"query_id\"]\n",
    "        query = example[\"query\"]\n",
    "        passages = example[\"passages\"]\n",
    "\n",
    "        for doc_idx, (text, label) in enumerate(\n",
    "            zip(passages[\"passage_text\"], passages[\"is_selected\"])\n",
    "        ):\n",
    "            rows.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"query\": query,\n",
    "                \"doc_id\": f\"{query_id}_{doc_idx}\",\n",
    "                \"document\": text,\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a195d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = flatten_dataset_split(dataset[\"train\"], CONFIG[\"n_queries\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718cecdb",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c9b80",
   "metadata": {},
   "source": [
    "The proposed system follows a multi-stage ranking architecture commonly employed in modern search engines.\n",
    "\n",
    "I use the MS MARCO Passage Ranking dataset, which provides query-centric data with binary relevance annotations. Each example consists of a query associated with multiple candidate passages and relevance labels. To enable ranking model training and evaluation, this nested structure is explicitly transformed into (query, document, label) pairs.\n",
    "\n",
    "In the first stage, I employ BM25 as a lexical retrieval model. For each query, BM25 is used to rank the associated passages and select the Top-K candidates. This stage acts as candidate generation, significantly reducing the search space and reflecting practical latency constraints found in real-world systems.\n",
    "\n",
    "In the second stage, I apply a Transformer-based cross-encoder for neural re-ranking. The cross-encoder jointly encodes each (query, document) pair and produces a relevance score. Two variants of this model are evaluated:\n",
    "\n",
    "1. A pre-trained cross-encoder, used directly in a zero-shot setting.\n",
    "\n",
    "2. The same model after supervised fine-tuning on the MS MARCO data using pointwise binary relevance labels.\n",
    "\n",
    "Evaluation is conducted using MRR@10 (Mean Reciprocal Rank) and NDCG@10 (Normalized Discounted Cumulative Gain), two standard metrics in information retrieval that emphasize correct ranking of relevant documents at top positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7403df7",
   "metadata": {},
   "source": [
    "## BM25 Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367122a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_tokenize(text: str):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.loIr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "755b763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_candidates(df: pd.DataFrame, top_k: int):\n",
    "    rows = []\n",
    "\n",
    "    for qid, group in df.groupby(\"query_id\"):\n",
    "        query = group[\"query\"].iloc[0]\n",
    "\n",
    "        tokenized_docs = [bm25_tokenize(d) for d in group[\"document\"]]\n",
    "        bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "        scores = bm25.get_scores(bm25_tokenize(query))\n",
    "\n",
    "        ranked = sorted(\n",
    "            zip(group.itertuples(), scores),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        for row, score in ranked[:top_k]:\n",
    "            rows.append({\n",
    "                \"query_id\": row.query_id,\n",
    "                \"query\": row.query,\n",
    "                \"doc_id\": row.doc_id,\n",
    "                \"document\": row.document,\n",
    "                \"label\": row.label,\n",
    "                \"bm25_score\": score\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d1d2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_df = bm25_candidates(train_df, CONFIG[\"bm25_top_k\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de352d",
   "metadata": {},
   "source": [
    "## Neural Re-ranking (Zero-shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec9d6b",
   "metadata": {},
   "source": [
    "Neural re-ranking is applied only on top-K BM25 candidates to emulate realistic search engine latency constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def110ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "cross_encoder = CrossEncoder(model_name, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c685dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 778/778 [00:04<00:00, 163.37it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs = list(zip(bm25_df[\"query\"], bm25_df[\"document\"]))\n",
    "\n",
    "bm25_df[\"neural_score\"] = cross_encoder.predict(\n",
    "    pairs,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58618d48",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5dbd8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr_at_k(df: pd.DataFrame, k: int) -> float:\n",
    "    rr = []\n",
    "    for _, group in df.groupby(\"query_id\"):\n",
    "        rel = group.head(k)[\"label\"].values\n",
    "        rr.append(1.0 / (np.where(rel == 1)[0][0] + 1) if 1 in rel else 0.0)\n",
    "    return float(np.mean(rr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a9b561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(df: pd.DataFrame, k: int) -> float:\n",
    "    scores = []\n",
    "\n",
    "    for _, group in df.groupby(\"query_id\"):\n",
    "        rel = group.head(k)[\"label\"].values\n",
    "        ideal = sorted(rel, reverse=True)\n",
    "\n",
    "        dcg = sum((2**r - 1) / np.log2(i + 2) for i, r in enumerate(rel))\n",
    "        idcg = sum((2**r - 1) / np.log2(i + 2) for i, r in enumerate(ideal))\n",
    "\n",
    "        scores.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "\n",
    "    return float(np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fded9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_ranked = bm25_df.sort_values([\"query_id\", \"bm25_score\"], ascending=[True, False])\n",
    "neural_ranked = bm25_df.sort_values([\"query_id\", \"neural_score\"], ascending=[True, False])\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"BM25\": {\n",
    "        \"MRR@10\": mrr_at_k(bm25_ranked, 10),\n",
    "        \"NDCG@10\": ndcg_at_k(bm25_ranked, 10),\n",
    "    },\n",
    "    \"Neural Re-ranking\": {\n",
    "        \"MRR@10\": mrr_at_k(neural_ranked, 10),\n",
    "        \"NDCG@10\": ndcg_at_k(neural_ranked, 10),\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977a30c",
   "metadata": {},
   "source": [
    "## Fine-tuning the Cross-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac85baf",
   "metadata": {},
   "source": [
    "Fine-tuning is performed using pointwise binary relevance supervision over BM25 candidates, which is a standard and stable approach for MS MARCO-style datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deb2496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    InputExample(texts=[row.query, row.document], label=float(row.label))\n",
    "    for row in bm25_df.itertuples()\n",
    "]\n",
    "\n",
    "train_ex, _ = train_test_split(train_examples, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "023b80bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7000' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7000/7000 02:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.323200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.100700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ft_model = CrossEncoder(model_name, num_labels=1, device=\"cuda\")\n",
    "\n",
    "ft_model.fit(\n",
    "    train_dataloader=DataLoader(train_ex, shuffle=True, batch_size=16),\n",
    "    epochs=CONFIG[\"epochs\"],\n",
    "    warmup_steps=1000,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "ft_model.save(\"./ce_finetuned\")\n",
    "ft_model = CrossEncoder(\"./ce_finetuned\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c7fcc",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c2902bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 778/778 [00:04<00:00, 169.99it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_df[\"ft_score\"] = ft_model.predict(pairs, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "ft_ranked = bm25_df.sort_values([\"query_id\", \"ft_score\"], ascending=[True, False])\n",
    "\n",
    "final_metrics = {\n",
    "    \"Fine-tuned Cross-Encoder\": {\n",
    "        \"MRR@10\": mrr_at_k(ft_ranked, 10),\n",
    "        \"NDCG@10\": ndcg_at_k(ft_ranked, 10),\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "247b0ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR@10</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BM25</th>\n",
       "      <td>0.3958</td>\n",
       "      <td>0.4837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Re-ranking (zero-shot)</th>\n",
       "      <td>0.5604</td>\n",
       "      <td>0.6084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fine-tuned Cross-Encoder</th>\n",
       "      <td>0.6997</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               MRR@10  NDCG@10\n",
       "BM25                           0.3958   0.4837\n",
       "Neural Re-ranking (zero-shot)  0.5604   0.6084\n",
       "Fine-tuned Cross-Encoder       0.6997   0.7125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"BM25\": {\n",
    "            \"MRR@10\": 0.3958,\n",
    "            \"NDCG@10\": 0.4837,\n",
    "        },\n",
    "        \"Neural Re-ranking (zero-shot)\": {\n",
    "            \"MRR@10\": 0.5604,\n",
    "            \"NDCG@10\": 0.6084,\n",
    "        },\n",
    "        \"Fine-tuned Cross-Encoder\": {\n",
    "            \"MRR@10\": 0.6997,\n",
    "            \"NDCG@10\": 0.7125,\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\"\n",
    ")\n",
    "results_df.style.format(\"{:.4f}\").highlight_max(axis=0)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ad8ce",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc026add",
   "metadata": {},
   "source": [
    "The results clearly demonstrate the effectiveness of neural re-ranking. While BM25 provides a strong lexical baseline, its performance is limited by its inability to capture semantic relationships beyond exact or near-exact term overlap.\n",
    "\n",
    "Applying a pre-trained cross-encoder in a zero-shot setting leads to a substantial improvement in both MRR@10 and NDCG@10, indicating that the model generalizes well to the search ranking task even without domain-specific training. This highlights the power of contextualized language representations for semantic matching.\n",
    "\n",
    "The largest performance gains are observed after fine-tuning the cross-encoder. Supervised training enables the model to learn dataset-specific relevance patterns, such as common query reformulations and salient semantic cues present in relevant passages. As a result, the fine-tuned model achieves significantly higher ranking quality, consistently placing relevant documents at top positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98beb192",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474af30c",
   "metadata": {},
   "source": [
    "This work presents a complete and realistic search ranking pipeline that integrates classical information retrieval techniques with modern NLP-based neural models. By combining BM25-based candidate generation with Transformer-based neural re-ranking, the proposed approach closely mirrors architectures deployed in large-scale search systems.\n",
    "\n",
    "Experimental results show that while BM25 remains an efficient and reliable baseline, neural re-ranking substantially improves ranking quality, and supervised fine-tuning further amplifies these gains. The improvements observed in MRR@10 and NDCG@10 confirm the effectiveness of the proposed pipeline and the importance of task-specific training for neural ranking models.\n",
    "\n",
    "Future work may explore pairwise or listwise training objectives, larger candidate sets in the first-stage retrieval, and analyses of computational cost and latency trade-offs. Nonetheless, the results obtained in this study demonstrate that neural re-ranking is a powerful and scalable approach for improving search relevance in NLP-driven systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
